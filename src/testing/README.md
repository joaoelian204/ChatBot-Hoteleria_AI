# üß™ M√≥dulo `testing/` - Pruebas y Validaci√≥n

## üìã Descripci√≥n General

El m√≥dulo `testing/` implementa un sistema completo de pruebas automatizadas para el ChatBot de Hoteler√≠a. Este m√≥dulo incluye pruebas unitarias, de integraci√≥n, de rendimiento y de funcionalidad para garantizar la calidad y confiabilidad del sistema.

## üèóÔ∏è Estructura del M√≥dulo

```
testing/
‚îú‚îÄ‚îÄ __init__.py              # üì¶ Inicializaci√≥n del m√≥dulo
‚îú‚îÄ‚îÄ config.py                # ‚öôÔ∏è Configuraci√≥n de pruebas
‚îú‚îÄ‚îÄ run_tests.py             # üöÄ Ejecutor de pruebas
‚îú‚îÄ‚îÄ test_suite.py            # üß™ Suite principal de pruebas
‚îî‚îÄ‚îÄ README.md                # üìñ Documentaci√≥n del m√≥dulo
```

## üìÑ Documentaci√≥n por Archivo

### ‚öôÔ∏è `config.py` - Configuraci√≥n de Pruebas

**Prop√≥sito**: Centraliza toda la configuraci√≥n relacionada con las pruebas, incluyendo entornos de testing, par√°metros de validaci√≥n y configuraciones espec√≠ficas.

**Estructura**:
- **TestConfig**: Clase principal de configuraci√≥n de pruebas
- **Entornos de Testing**: Configuraci√≥n para diferentes entornos
- **Par√°metros de Validaci√≥n**: Criterios de √©xito para pruebas
- **Configuraci√≥n de Modelos**: Setup para modelos de testing

**Funcionalidades**:
- ‚úÖ Configuraci√≥n de entornos de testing (desarrollo, staging, producci√≥n)
- ‚úÖ Par√°metros de validaci√≥n personalizables
- ‚úÖ Configuraci√≥n de modelos dummy para pruebas
- ‚úÖ Timeouts y l√≠mites de tiempo
- ‚úÖ Configuraci√≥n de datos de prueba
- ‚úÖ Logging espec√≠fico para pruebas
- ‚úÖ Configuraci√≥n de coverage y reportes
- ‚úÖ Integraci√≥n con CI/CD

**Estructura de Configuraci√≥n**:
```python
class TestConfig:
    def __init__(self):
        # Configuraci√≥n b√°sica de pruebas
        self.TEST_ENVIRONMENT = "development"
        self.USE_DUMMY_MODELS = True
        self.TEST_TIMEOUT = 30
        self.VALIDATION_THRESHOLD = 0.8
        
        # Configuraci√≥n de datos de prueba
        self.TEST_DATA_DIR = "test_data/"
        self.SAMPLE_QUERIES = [...]
        self.EXPECTED_RESPONSES = {...}
        
        # Configuraci√≥n de reporting
        self.GENERATE_REPORTS = True
        self.COVERAGE_REPORT = True
        self.HTML_REPORTS = True
```

**Configuraciones por Entorno**:

#### üõ†Ô∏è Desarrollo
```python
TEST_ENVIRONMENT = "development"
USE_DUMMY_MODELS = True
TEST_TIMEOUT = 10
VALIDATION_THRESHOLD = 0.7
GENERATE_REPORTS = False
```

#### üß™ Staging
```python
TEST_ENVIRONMENT = "staging"
USE_DUMMY_MODELS = False
TEST_TIMEOUT = 30
VALIDATION_THRESHOLD = 0.8
GENERATE_REPORTS = True
```

#### üöÄ Producci√≥n
```python
TEST_ENVIRONMENT = "production"
USE_DUMMY_MODELS = False
TEST_TIMEOUT = 60
VALIDATION_THRESHOLD = 0.9
GENERATE_REPORTS = True
```

**Uso**:
```python
from testing.config import TestConfig

# Crear configuraci√≥n de pruebas
config = TestConfig()

# Verificar configuraci√≥n
if config.USE_DUMMY_MODELS:
    print("üß™ Usando modelos dummy para pruebas")
else:
    print("üöÄ Usando modelos reales para pruebas")

# Obtener par√°metros de validaci√≥n
threshold = config.VALIDATION_THRESHOLD
timeout = config.TEST_TIMEOUT
```

**Dependencias**:
- `os`: Variables de entorno
- `pathlib`: Manejo de rutas
- `config.settings`: Configuraci√≥n del sistema

---

### üöÄ `run_tests.py` - Ejecutor de Pruebas

**Prop√≥sito**: Proporciona una interfaz de l√≠nea de comandos y program√°tica para ejecutar todas las pruebas del sistema de manera organizada y configurable.

**Estructura**:
- **TestRunner**: Clase principal para ejecuci√≥n de pruebas
- **Argumentos de L√≠nea de Comandos**: Interfaz CLI
- **Ejecuci√≥n Program√°tica**: API para ejecutar pruebas
- **Reportes**: Generaci√≥n de reportes de resultados

**Funcionalidades**:
- ‚úÖ Ejecuci√≥n de pruebas desde l√≠nea de comandos
- ‚úÖ Ejecuci√≥n program√°tica de suites de pruebas
- ‚úÖ Filtrado de pruebas por categor√≠a
- ‚úÖ Configuraci√≥n de entornos de testing
- ‚úÖ Generaci√≥n de reportes detallados
- ‚úÖ Integraci√≥n con sistemas CI/CD
- ‚úÖ Manejo de errores y timeouts
- ‚úÖ Paralelizaci√≥n de pruebas
- ‚úÖ Coverage reporting
- ‚úÖ Notificaciones de resultados

**Interfaz de L√≠nea de Comandos**:
```bash
# Ejecutar todas las pruebas
python src/testing/run_tests.py

# Ejecutar pruebas espec√≠ficas
python src/testing/run_tests.py --category unit
python src/testing/run_tests.py --category integration
python src/testing/run_tests.py --category performance

# Configurar entorno
python src/testing/run_tests.py --environment staging
python src/testing/run_tests.py --environment production

# Generar reportes
python src/testing/run_tests.py --generate-reports
python src/testing/run_tests.py --coverage-report

# Configurar timeouts
python src/testing/run_tests.py --timeout 60
python src/testing/run_tests.py --parallel
```

**Uso Program√°tico**:
```python
from testing.run_tests import TestRunner

# Crear ejecutor de pruebas
runner = TestRunner()

# Ejecutar todas las pruebas
results = runner.run_all_tests()

# Ejecutar categor√≠a espec√≠fica
unit_results = runner.run_tests_by_category("unit")

# Ejecutar con configuraci√≥n personalizada
custom_results = runner.run_tests(
    categories=["unit", "integration"],
    environment="staging",
    timeout=60,
    generate_reports=True
)

# Verificar resultados
if results.success:
    print(f"‚úÖ Todas las pruebas pasaron: {results.passed}/{results.total}")
else:
    print(f"‚ùå Algunas pruebas fallaron: {results.failed}/{results.total}")
```

**Funciones Principales**:

#### `run_all_tests()`
- **Prop√≥sito**: Ejecuta todas las pruebas disponibles
- **Retorna**: Objeto con resultados completos
- **Funcionalidad**:
  - Ejecuta pruebas unitarias
  - Ejecuta pruebas de integraci√≥n
  - Ejecuta pruebas de rendimiento
  - Genera reportes consolidados

#### `run_tests_by_category(category)`
- **Prop√≥sito**: Ejecuta pruebas de una categor√≠a espec√≠fica
- **Par√°metros**:
  - `category`: Categor√≠a de pruebas ("unit", "integration", "performance")
- **Retorna**: Resultados de la categor√≠a espec√≠fica

#### `run_tests(categories, environment, timeout, generate_reports)`
- **Prop√≥sito**: Ejecuta pruebas con configuraci√≥n personalizada
- **Par√°metros**:
  - `categories`: Lista de categor√≠as a ejecutar
  - `environment`: Entorno de testing
  - `timeout`: Timeout en segundos
  - `generate_reports`: Si generar reportes
- **Retorna**: Resultados personalizados

**Dependencias**:
- `argparse`: Argumentos de l√≠nea de comandos
- `unittest`: Framework de pruebas
- `pytest`: Framework de pruebas avanzado
- `coverage`: An√°lisis de cobertura de c√≥digo
- `testing.config`: Configuraci√≥n de pruebas
- `testing.test_suite`: Suite de pruebas

---

### üß™ `test_suite.py` - Suite Principal de Pruebas

**Prop√≥sito**: Contiene todas las pruebas automatizadas del sistema, organizadas por categor√≠as y funcionalidades espec√≠ficas.

**Estructura**:
- **TestBase**: Clase base para todas las pruebas
- **UnitTests**: Pruebas unitarias de componentes individuales
- **IntegrationTests**: Pruebas de integraci√≥n entre m√≥dulos
- **PerformanceTests**: Pruebas de rendimiento y escalabilidad
- **FunctionalTests**: Pruebas de funcionalidad end-to-end

**Categor√≠as de Pruebas**:

#### üß© Pruebas Unitarias
- **AI Models**: Pruebas de modelos de IA
- **Vectorstore**: Pruebas de base de conocimiento
- **Cache System**: Pruebas del sistema de cach√©
- **Text Processing**: Pruebas de procesamiento de texto
- **Configuration**: Pruebas de configuraci√≥n del sistema

#### üîó Pruebas de Integraci√≥n
- **Bot Integration**: Pruebas de integraci√≥n del bot
- **AI Pipeline**: Pruebas del pipeline completo de IA
- **Analytics Integration**: Pruebas del sistema de analytics
- **Training Integration**: Pruebas del sistema de entrenamiento

#### ‚ö° Pruebas de Rendimiento
- **Response Time**: Pruebas de tiempo de respuesta
- **Memory Usage**: Pruebas de uso de memoria
- **Concurrency**: Pruebas de concurrencia
- **Scalability**: Pruebas de escalabilidad

#### üéØ Pruebas Funcionales
- **User Interactions**: Pruebas de interacciones de usuario
- **Command Processing**: Pruebas de procesamiento de comandos
- **Error Handling**: Pruebas de manejo de errores
- **Edge Cases**: Pruebas de casos l√≠mite

**Estructura de Clases**:
```python
class TestBase:
    """Clase base para todas las pruebas"""
    def setUp(self):
        # Configuraci√≥n inicial de pruebas
    
    def tearDown(self):
        # Limpieza despu√©s de pruebas

class AIModelsTests(TestBase):
    """Pruebas de modelos de IA"""
    def test_model_loading(self):
        # Prueba carga de modelos
    
    def test_model_inference(self):
        # Prueba inferencia de modelos

class BotIntegrationTests(TestBase):
    """Pruebas de integraci√≥n del bot"""
    def test_message_processing(self):
        # Prueba procesamiento de mensajes
    
    def test_command_handling(self):
        # Prueba manejo de comandos

class PerformanceTests(TestBase):
    """Pruebas de rendimiento"""
    def test_response_time(self):
        # Prueba tiempo de respuesta
    
    def test_memory_usage(self):
        # Prueba uso de memoria
```

**Ejemplos de Pruebas**:

#### Prueba de Carga de Modelos
```python
def test_model_loading(self):
    """Prueba que los modelos se carguen correctamente"""
    from ai.models import ai_models
    
    # Verificar que los modelos est√©n disponibles
    self.assertIsNotNone(ai_models.get_generador_sync())
    self.assertIsNotNone(ai_models.get_embedding_model_sync())
    
    # Verificar estad√≠sticas
    stats = ai_models.get_stats()
    self.assertIn('models_loaded', stats)
    self.assertGreater(stats['models_loaded'], 0)
```

#### Prueba de Procesamiento de Mensajes
```python
def test_message_processing(self):
    """Prueba el procesamiento completo de mensajes"""
    from bot.bot_main import HoteleriaBot
    
    bot = HoteleriaBot()
    
    # Simular mensaje de usuario
    test_message = "¬øQu√© habitaciones tienen disponibles?"
    
    # Procesar mensaje
    response = bot._get_ai_response(test_message)
    
    # Verificar respuesta
    self.assertIsNotNone(response)
    self.assertGreater(len(response), 0)
    self.assertIn("habitaci√≥n", response.lower())
```

#### Prueba de Rendimiento
```python
def test_response_time(self):
    """Prueba que el tiempo de respuesta sea aceptable"""
    import time
    from ai.text_generator import generate_response
    
    start_time = time.time()
    
    # Generar respuesta
    response = await generate_response("Consulta de prueba")
    
    end_time = time.time()
    response_time = end_time - start_time
    
    # Verificar tiempo de respuesta
    self.assertLess(response_time, 5.0)  # M√°ximo 5 segundos
    self.assertIsNotNone(response)
```

**Dependencias**:
- `unittest`: Framework de pruebas
- `asyncio`: Pruebas as√≠ncronas
- `time`: Medici√≥n de tiempo
- `ai.models`: Modelos de IA
- `ai.vectorstore`: Base de conocimiento
- `bot.bot_main`: Bot principal
- `config.settings`: Configuraci√≥n del sistema

---

## üîß Caracter√≠sticas del M√≥dulo

### üéØ Cobertura Completa
- **Pruebas Unitarias**: Cobertura de componentes individuales
- **Pruebas de Integraci√≥n**: Verificaci√≥n de interacciones entre m√≥dulos
- **Pruebas de Rendimiento**: Validaci√≥n de escalabilidad y velocidad
- **Pruebas Funcionales**: Verificaci√≥n de funcionalidad end-to-end

### ‚ö° Ejecuci√≥n Eficiente
- **Paralelizaci√≥n**: Ejecuci√≥n concurrente de pruebas independientes
- **Filtrado Inteligente**: Ejecuci√≥n selectiva por categor√≠a
- **Timeouts Configurables**: Control de tiempo de ejecuci√≥n
- **Recursos Optimizados**: Uso eficiente de memoria y CPU

### üìä Reportes Detallados
- **Reportes HTML**: Visualizaci√≥n interactiva de resultados
- **Coverage Reports**: An√°lisis de cobertura de c√≥digo
- **M√©tricas de Rendimiento**: Estad√≠sticas de tiempo y recursos
- **Logs Detallados**: Informaci√≥n para debugging

### üîÑ Integraci√≥n CI/CD
- **Automatizaci√≥n**: Ejecuci√≥n autom√°tica en pipelines
- **Notificaciones**: Alertas de fallos en pruebas
- **Thresholds**: Criterios de √©xito configurables
- **Rollback**: Reversi√≥n autom√°tica en caso de fallos

## üöÄ Configuraci√≥n del Sistema

### Variables de Entorno
```bash
# Configuraci√≥n de Pruebas
TEST_ENVIRONMENT=development
USE_DUMMY_MODELS=true
TEST_TIMEOUT=30
VALIDATION_THRESHOLD=0.8

# Configuraci√≥n de Reportes
GENERATE_REPORTS=true
COVERAGE_REPORT=true
HTML_REPORTS=true

# Configuraci√≥n de CI/CD
CI_MODE=false
NOTIFY_ON_FAILURE=true
AUTO_ROLLBACK_ON_FAILURE=false
```

### Configuraci√≥n por Entorno

#### Desarrollo
```bash
TEST_ENVIRONMENT=development
USE_DUMMY_MODELS=true
TEST_TIMEOUT=10
GENERATE_REPORTS=false
```

#### Staging
```bash
TEST_ENVIRONMENT=staging
USE_DUMMY_MODELS=false
TEST_TIMEOUT=30
GENERATE_REPORTS=true
```

#### Producci√≥n
```bash
TEST_ENVIRONMENT=production
USE_DUMMY_MODELS=false
TEST_TIMEOUT=60
GENERATE_REPORTS=true
AUTO_ROLLBACK_ON_FAILURE=true
```

## üìÅ Estructura de Reportes

### Archivos de Pruebas
```
testing/
‚îú‚îÄ‚îÄ reports/                 # üìã Reportes de pruebas
‚îÇ   ‚îú‚îÄ‚îÄ html/                # Reportes HTML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_results.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ coverage_report.html
‚îÇ   ‚îú‚îÄ‚îÄ json/                # Reportes JSON
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_results.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ performance_metrics.json
‚îÇ   ‚îî‚îÄ‚îÄ logs/                # Logs de pruebas
‚îÇ       ‚îú‚îÄ‚îÄ test_execution.log
‚îÇ       ‚îî‚îÄ‚îÄ error_logs.log
‚îú‚îÄ‚îÄ test_data/               # üìä Datos de prueba
‚îÇ   ‚îú‚îÄ‚îÄ sample_queries.txt
‚îÇ   ‚îú‚îÄ‚îÄ expected_responses.json
‚îÇ   ‚îî‚îÄ‚îÄ test_documents/
‚îî‚îÄ‚îÄ coverage/                # üìà Datos de cobertura
    ‚îú‚îÄ‚îÄ .coverage
    ‚îî‚îÄ‚îÄ htmlcov/
```

## üöÄ Inicio R√°pido

```bash
# Ejecutar todas las pruebas
python src/testing/run_tests.py

# Ejecutar pruebas espec√≠ficas
python src/testing/run_tests.py --category unit

# Ejecutar con configuraci√≥n personalizada
python src/testing/run_tests.py --environment staging --timeout 60

# Generar reportes
python src/testing/run_tests.py --generate-reports --coverage-report
```

```python
# Ejecuci√≥n program√°tica
from testing.run_tests import TestRunner

runner = TestRunner()
results = runner.run_all_tests()

if results.success:
    print("‚úÖ Todas las pruebas pasaron")
else:
    print(f"‚ùå {results.failed} pruebas fallaron")
```

## üìù Notas de Desarrollo

- **Cobertura**: Mantener cobertura de c√≥digo > 80%
- **Pruebas Regulares**: Ejecutar pruebas antes de cada commit
- **Documentaci√≥n**: Mantener pruebas actualizadas con cambios
- **Performance**: Monitorear tiempo de ejecuci√≥n de pruebas
- **Debugging**: Usar logs detallados para debugging
- **CI/CD**: Integrar pruebas en pipeline de deployment
