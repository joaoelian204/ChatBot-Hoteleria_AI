# ğŸ¤– MÃ³dulo `ai/` - Inteligencia Artificial

## ğŸ“‹ DescripciÃ³n General

El mÃ³dulo `ai/` contiene todos los componentes relacionados con la Inteligencia Artificial del ChatBot de HotelerÃ­a. Este mÃ³dulo implementa un sistema completo de IA que incluye modelos de lenguaje, procesamiento de texto, bÃºsqueda semÃ¡ntica y gestiÃ³n de recursos.

## ğŸ—ï¸ Estructura del MÃ³dulo

```
ai/
â”œâ”€â”€ __init__.py              # ğŸ“¦ InicializaciÃ³n del mÃ³dulo
â”œâ”€â”€ models.py                # ğŸ§  GestiÃ³n de modelos de IA
â”œâ”€â”€ vectorstore.py           # ğŸ“š Base de conocimiento vectorial
â”œâ”€â”€ intent_detector.py       # ğŸ¯ DetecciÃ³n de intenciones
â”œâ”€â”€ text_generator.py        # âœï¸ GeneraciÃ³n de respuestas
â”œâ”€â”€ cache.py                 # ğŸ’¾ Sistema de cachÃ©
â”œâ”€â”€ resource_manager.py      # âš™ï¸ GestiÃ³n de recursos
â””â”€â”€ fallback_handler.py      # ğŸ›¡ï¸ Manejo de fallbacks
```

## ğŸ“„ DocumentaciÃ³n por Archivo

### ğŸ§  `models.py` - GestiÃ³n de Modelos de IA

**PropÃ³sito**: Centraliza la gestiÃ³n de todos los modelos de IA utilizados en el sistema.

**Estructura**:
- **ModelFactory**: Factory pattern para crear instancias de modelos
- **AIModels**: Clase principal que gestiona todos los modelos
- **Lazy Loading**: Sistema de carga perezosa para optimizar memoria

**Funcionalidades**:
- âœ… Carga de modelos de resumen (BERT2BERT)
- âœ… Carga de modelos de generaciÃ³n (GPT-2 Spanish)
- âœ… Carga de modelos de embeddings (Sentence Transformers)
- âœ… Carga de clasificadores de intenciones (BART)
- âœ… GestiÃ³n de recursos con carga perezosa
- âœ… Fallback a modelos dummy en caso de error

**Uso**:
```python
from ai.models import ai_models

# Obtener modelo de generaciÃ³n (async)
generator = await ai_models.get_generador()

# Obtener modelo de embeddings (async)
embeddings = await ai_models.get_embedding_model()

# Obtener estadÃ­sticas
stats = ai_models.get_stats()
```

**Dependencias**:
- `torch`: Framework de machine learning
- `transformers`: Modelos de Hugging Face
- `langchain_huggingface`: IntegraciÃ³n con LangChain
- `config.settings`: ConfiguraciÃ³n del sistema

---

### ğŸ“š `vectorstore.py` - Base de Conocimiento Vectorial

**PropÃ³sito**: Gestiona la base de conocimiento vectorial para bÃºsquedas semÃ¡nticas en documentos del hotel.

**Estructura**:
- **VectorStoreManager**: Clase principal para gestiÃ³n del vectorstore
- **Carga de Documentos**: Soporte para PDF, TXT, DOCX
- **BÃºsqueda SemÃ¡ntica**: BÃºsqueda por similitud vectorial
- **GestiÃ³n de Chunks**: DivisiÃ³n inteligente de documentos

**Funcionalidades**:
- âœ… Carga de mÃºltiples formatos de documentos
- âœ… DivisiÃ³n automÃ¡tica en chunks optimizados
- âœ… BÃºsqueda semÃ¡ntica con FAISS
- âœ… ActualizaciÃ³n dinÃ¡mica de conocimiento
- âœ… GestiÃ³n de memoria con lazy loading
- âœ… EstadÃ­sticas de uso y rendimiento

**Uso**:
```python
from ai.vectorstore import vectorstore_manager

# BÃºsqueda sÃ­ncrona
results = vectorstore_manager.search_context("Â¿QuÃ© habitaciones tienen?", k=3)

# BÃºsqueda asÃ­ncrona (recomendado)
results = await vectorstore_manager.search_context_async("Â¿CuÃ¡l es el precio?", k=3)

# Actualizar conocimiento
vectorstore_manager.update_knowledge()

# AÃ±adir documento especÃ­fico
vectorstore_manager.add_document("nuevo_documento.txt")
```

**Dependencias**:
- `faiss`: BÃºsqueda vectorial eficiente
- `langchain`: Framework para aplicaciones de IA
- `ai.models`: Modelos de embeddings

---

### ğŸ¯ `intent_detector.py` - DetecciÃ³n de Intenciones

**PropÃ³sito**: Clasifica automÃ¡ticamente las intenciones de los usuarios para proporcionar respuestas mÃ¡s precisas.

**Estructura**:
- **DetecciÃ³n de Intenciones**: ClasificaciÃ³n automÃ¡tica de consultas
- **Mapeo de Intenciones**: TraducciÃ³n de intenciones a acciones
- **Fallback Inteligente**: Manejo de casos no reconocidos

**Funcionalidades**:
- âœ… ClasificaciÃ³n automÃ¡tica de intenciones
- âœ… Soporte para mÃºltiples categorÃ­as (habitaciones, restaurantes, etc.)
- âœ… Fallback inteligente para casos no reconocidos
- âœ… IntegraciÃ³n con modelos de IA
- âœ… Cache de clasificaciones frecuentes

**Uso**:
```python
from ai.intent_detector import detect_intent

# Detectar intenciÃ³n
intent = detect_intent("Â¿QuÃ© habitaciones tienen disponibles?")
# Resultado: "habitaciones"

# Con contexto adicional
intent = detect_intent("Â¿CuÃ¡l es el precio de la suite?", context="precios")
# Resultado: "precios"
```

**Dependencias**:
- `ai.models`: Modelos de clasificaciÃ³n
- `ai.cache`: Sistema de cachÃ©

---

### âœï¸ `text_generator.py` - GeneraciÃ³n de Respuestas

**PropÃ³sito**: Genera respuestas contextuales y naturales basadas en la informaciÃ³n del hotel y las consultas de los usuarios.

**Estructura**:
- **GeneraciÃ³n Contextual**: Respuestas basadas en contexto relevante
- **Formateo de Respuestas**: AplicaciÃ³n de formato y estilo
- **ValidaciÃ³n de Calidad**: VerificaciÃ³n de respuestas generadas

**Funcionalidades**:
- âœ… GeneraciÃ³n de respuestas contextuales
- âœ… IntegraciÃ³n con base de conocimiento
- âœ… Formateo automÃ¡tico de respuestas
- âœ… ValidaciÃ³n de calidad de respuestas
- âœ… Soporte para mÃºltiples idiomas
- âœ… PersonalizaciÃ³n segÃºn tipo de consulta

**Uso**:
```python
from ai.text_generator import generate_response

# Generar respuesta simple
response = await generate_response("Â¿QuÃ© habitaciones tienen?")

# Generar respuesta con contexto especÃ­fico
response = await generate_response(
    "Â¿CuÃ¡l es el precio?", 
    context="habitaciones_precios.txt"
)
```

**Dependencias**:
- `ai.models`: Modelos de generaciÃ³n
- `ai.vectorstore`: Base de conocimiento
- `ai.intent_detector`: DetecciÃ³n de intenciones

---

### ğŸ’¾ `cache.py` - Sistema de CachÃ©

**PropÃ³sito**: Optimiza el rendimiento del sistema mediante el almacenamiento temporal de respuestas frecuentes.

**Estructura**:
- **Cache de Respuestas**: Almacenamiento de respuestas generadas
- **Cache de Modelos**: Cache de modelos cargados
- **GestiÃ³n de Memoria**: Control automÃ¡tico del tamaÃ±o del cache
- **PolÃ­ticas de EvicciÃ³n**: LRU para gestiÃ³n de memoria

**Funcionalidades**:
- âœ… Cache de respuestas con TTL configurable
- âœ… Cache de modelos para evitar recarga
- âœ… GestiÃ³n automÃ¡tica de memoria
- âœ… EstadÃ­sticas de rendimiento
- âœ… Limpieza automÃ¡tica de cache expirado
- âœ… PolÃ­tica LRU para evicciÃ³n

**Uso**:
```python
from ai.cache import response_cache

# Obtener respuesta del cache
response = response_cache.get("clave_consulta")

# Almacenar respuesta en cache
response_cache.set("clave_consulta", "respuesta", ttl=3600)

# Obtener estadÃ­sticas
stats = response_cache.get_stats()
```

**Dependencias**:
- `config.settings`: ConfiguraciÃ³n de cache
- `utils.logger`: Sistema de logging

---

### âš™ï¸ `resource_manager.py` - GestiÃ³n de Recursos

**PropÃ³sito**: Gestiona eficientemente los recursos del sistema, especialmente la memoria y la carga de modelos pesados.

**Estructura**:
- **GestiÃ³n de Memoria**: Control del uso de RAM
- **Carga Lazy**: Carga de modelos solo cuando es necesario
- **Control de Concurrencia**: GestiÃ³n de mÃºltiples requests simultÃ¡neos
- **Monitoreo de Recursos**: Seguimiento del uso de recursos

**Funcionalidades**:
- âœ… Carga perezosa de modelos pesados
- âœ… Control de concurrencia
- âœ… GestiÃ³n automÃ¡tica de memoria
- âœ… Monitoreo de recursos en tiempo real
- âœ… Limpieza automÃ¡tica de recursos no utilizados
- âœ… Fallback en caso de escasez de recursos

**Uso**:
```python
from ai.resource_manager import resource_manager

# Registrar modelo para carga perezosa
resource_manager.register_model("mi_modelo", create_function)

# Obtener modelo (se carga si es necesario)
model = await resource_manager.get_model("mi_modelo")

# Obtener estadÃ­sticas de recursos
stats = resource_manager.get_stats()
```

**Dependencias**:
- `config.settings`: ConfiguraciÃ³n de recursos
- `utils.logger`: Sistema de logging

---

### ğŸ›¡ï¸ `fallback_handler.py` - Manejo de Fallbacks

**PropÃ³sito**: Proporciona respuestas de respaldo cuando los modelos principales fallan o no estÃ¡n disponibles.

**Estructura**:
- **Respuestas de Fallback**: Respuestas predefinidas para casos de error
- **DetecciÃ³n de Errores**: IdentificaciÃ³n automÃ¡tica de fallos
- **RecuperaciÃ³n**: Mecanismos de recuperaciÃ³n automÃ¡tica
- **Logging de Errores**: Registro detallado de fallos

**Funcionalidades**:
- âœ… Respuestas de fallback predefinidas
- âœ… DetecciÃ³n automÃ¡tica de errores
- âœ… RecuperaciÃ³n automÃ¡tica de servicios
- âœ… Logging detallado de errores
- âœ… Respuestas contextuales de fallback
- âœ… Notificaciones de estado del sistema

**Uso**:
```python
from ai.fallback_handler import handle_fallback

# Manejar fallback automÃ¡ticamente
response = await handle_fallback("consulta_usuario", error_type="model_error")

# Obtener respuesta de fallback especÃ­fica
fallback_response = get_fallback_response("habitaciones")
```

**Dependencias**:
- `config.settings`: ConfiguraciÃ³n del sistema
- `utils.logger`: Sistema de logging

---

## ğŸ”§ CaracterÃ­sticas del MÃ³dulo

### âš¡ OptimizaciÃ³n de Rendimiento
- **Lazy Loading**: Carga de modelos solo cuando es necesario
- **Sistema de Cache**: Almacenamiento temporal de respuestas
- **GestiÃ³n de Memoria**: Control eficiente de recursos
- **BÃºsqueda Vectorial**: FAISS para bÃºsquedas rÃ¡pidas

### ğŸ”’ Robustez y Confiabilidad
- **Fallbacks Inteligentes**: Respuestas de respaldo automÃ¡ticas
- **Manejo de Errores**: GestiÃ³n centralizada de excepciones
- **ValidaciÃ³n**: VerificaciÃ³n de calidad de respuestas
- **RecuperaciÃ³n**: Mecanismos de recuperaciÃ³n automÃ¡tica

### ğŸ“ˆ Escalabilidad
- **Arquitectura Modular**: FÃ¡cil adiciÃ³n de nuevos modelos
- **ConfiguraciÃ³n Flexible**: Ajustes mediante variables de entorno
- **Monitoreo**: Seguimiento de rendimiento y recursos
- **Testing**: Pruebas automatizadas para cada componente

## ğŸš€ Inicio RÃ¡pido

```python
# Importar mÃ³dulo completo
from ai import *

# Usar modelos de IA
generator = await ai_models.get_generador()
response = await generate_response("Â¿QuÃ© servicios ofrecen?")

# Usar vectorstore
results = await vectorstore_manager.search_context_async("habitaciones")

# Usar cache
cached_response = response_cache.get("mi_consulta")
```

## ğŸ“ Notas de Desarrollo

- **Lazy Loading**: Habilitado por defecto para optimizar memoria
- **Cache**: Configurable mediante variables de entorno
- **Logging**: Todos los componentes usan el logger centralizado
- **Testing**: Cada componente tiene pruebas unitarias
- **DocumentaciÃ³n**: Mantener docstrings actualizados 